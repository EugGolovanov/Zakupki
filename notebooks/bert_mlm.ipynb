{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa34d6e5-acf0-42ee-908b-66d5d0a5ebd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e415b04-2c98-4e9d-92c4-d4c3e136cc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import gc\n",
    "import scipy.io as sio\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from simpletransformers.language_modeling import (\n",
    "    LanguageModelingModel,\n",
    "    LanguageModelingArgs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f45c0c1e-a99b-4b14-9d9d-1164decc21e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Oct  1 12:53:40 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.172.01   Driver Version: 450.172.01   CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  A100-PCIE-40GB      On   | 00000000:CA:00.0 Off |                    0 |\n",
      "| N/A   28C    P0    40W / 250W |      0MiB / 40537MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fc189b3-d1da-471f-a957-136ad55e728f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07c683f7-3e04-4992-87c9-5a8014366122",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_cache():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    return f\"GPU memory occupied: {info.used//1024**2} MB.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46f05f7c-be0b-4605-99ce-65dfc8400cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"\n",
    "    model: torch.nn.Module\n",
    "    \n",
    "    Используется для подробного вывода\n",
    "    параметров модели\n",
    "    \"\"\"\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad:\n",
    "            print(name, parameter.numel())\n",
    "            continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params += params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a9d7a6a-235f-449f-982d-a940671dfaeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny2 were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")\n",
    "model = AutoModel.from_pretrained(\"cointegrated/rubert-tiny2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35b466b7-16d9-4d9d-998e-c6c1066751ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5068a320-7173-418a-bc23-ecb0c99fd09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfK = pd.read_csv('./data/Контракты 44ФЗ.csv', sep=';').drop_duplicates()\n",
    "dfD = pd.read_csv('./data/Справочник пром производства.csv', sep=';').drop_duplicates()\n",
    "dfP = pd.read_csv('./data/Ценовые предложения поставщиков.csv', sep=';').drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bfbe399-056a-4f94-b03c-df0997615997",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfK.is_contract = 1\n",
    "dfP.is_contract = -1\n",
    "dfD.is_contract = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "515befa6-fac1-4654-866f-8e1a93913909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((916112, 9), (222713, 9), (105460, 9))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfK.shape, dfP.shape, dfD.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e880303a-21e9-4ba4-9915-f890fc8d845c",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.concat([dfK, dfP, dfD], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ef787bf-1f64-4ad9-910c-4f89e82a1074",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = full_df[[\"product_name\", \"okpd2_code\", \"product_characteristics\"]].drop_duplicates()\n",
    "count_df = full_df[[\"product_name\", \"okpd2_code\", \"product_characteristics\"]].drop_duplicates().groupby(\"okpd2_code\").agg(\"count\").reset_index()\n",
    "classes = count_df.query(\"product_name >= 100\")[\"okpd2_code\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "981398e7-c7f1-4610-8856-a42b6d8f4f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertCLS(nn.Module):\n",
    "    def __init__(self, model, n_classes):\n",
    "        super(BertCLS, self).__init__()\n",
    "        self.model = model\n",
    "        self.fc = nn.Linear(312, n_classes)\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        return self.fc(self.model(**batch).pooler_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f995127e-483e-412f-a259-57e91e6149f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_cls = BertCLS(model, len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "748eb682-fcac-46c4-89eb-897737f71977",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected2 = selected[selected[\"okpd2_code\"].isin(classes)].reset_index(drop=True)\n",
    "selected2[\"product_characteristics\"] = selected2[\"product_characteristics\"].replace(np.nan, \"\")\n",
    "selected2[\"text\"] = (selected2[\"product_name\"].str.strip() + \" [SEP] \" + selected2[\"product_characteristics\"].str.strip()).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85252200-a78c-4a09-a631-2ebb0cacc527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.32.30.390    142367\n",
       "45.20.11.519     58569\n",
       "58.11.11.000     38223\n",
       "58.11.19.000     29122\n",
       "32.50.50.190     19832\n",
       "                 ...  \n",
       "32.50.50.181       101\n",
       "10.20.25.111       100\n",
       "29.32.30.211       100\n",
       "33.13.12.000       100\n",
       "28.11.41.000       100\n",
       "Name: okpd2_code, Length: 1005, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected2[\"okpd2_code\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81fe0db2-b8d9-4016-bdab-1c36526f52fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "selected2[\"target\"] = le.fit_transform(selected2[\"okpd2_code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f2b58f2e-65b5-4daa-821a-54dcfcaa2bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(selected2[[\"text\", \"target\"]].values,\n",
    "                                   test_size=0.1,\n",
    "                                   random_state=42,\n",
    "                                   stratify=selected2[\"target\"],\n",
    "                                   shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b6870508-f328-4766-b789-e0efe02e9478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(839439, 2)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0766d174-022a-44c2-ab3e-7d3af842826a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(93271, 2)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5b139d72-77d7-4611-b2d2-f856d74e5918",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(X_train[:,0]).to_csv(\"data/train.txt\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "96e9da39-6dde-4a5b-9e8c-5eeaf3683b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(X_test[:,0]).to_csv(\"data/test.txt\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96ba801-4a6c-4074-9473-28bde265b278",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2243ad727b554735ac9f2fca671275ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81657e4ef8b344f6b67f9c6e8a0cf52b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dd4e149547e4235b28461203559bc03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/401 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b9b213f4c8643cf962dfd8cb8bc4ea4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/715 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5e4a2dedebd4487ae978117577aab76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/118M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny2 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "INFO:simpletransformers.language_modeling.language_modeling_utils: Creating features from dataset file at cache_dir/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ea01367b29b44268b3fc9781f041774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/839440 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3135 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2395 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2397 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5475 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3362 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2401 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2140 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3650 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3573 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (7771 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3193 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4228 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2140 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2140 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2392 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4539 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2140 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc1ff6d216cd4f018d22fa6b851e5f53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/448285 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.language_modeling.language_modeling_utils: Saving features into cached file cache_dir/bert_cached_lm_126_train.txt\n",
      "INFO:simpletransformers.language_modeling.language_modeling_model: Training started\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0e6534536cb4105aa740f964204dc25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eddbef4f3f69434d818d6cfbcc2f2446",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 10:   0%|          | 0/56036 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)\n",
    "\n",
    "model_args = LanguageModelingArgs()\n",
    "model_args.reprocess_input_data = True\n",
    "model_args.overwrite_output_dir = True\n",
    "model_args.num_train_epochs = 10\n",
    "model_args.dataset_type = \"simple\"\n",
    "\n",
    "train_file = \"data/train.txt\"\n",
    "test_file = \"data/test.txt\"\n",
    "\n",
    "model = LanguageModelingModel(\n",
    "    \"bert\", \"cointegrated/rubert-tiny2\", args=model_args\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.train_model(train_file, eval_file=test_file)\n",
    "\n",
    "# Evaluate the model\n",
    "result = model.eval_model(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308f6e59-1f63-4007-9b64-06e14540e485",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1ea38722-69ff-45dc-919e-b30c009ff5b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.0903824446277521, 'perplexity': tensor(2.9754)}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45764af-afd8-4bd4-8a02-1c63e40ef4fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
